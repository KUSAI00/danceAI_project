{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69c2538-be35-4e08-8488-49e61445f376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "# CHANGED: Support both Hugging Face and Google Gemini\n",
    "from huggingface_hub import InferenceClient\n",
    "try:\n",
    "    import google.generativeai as genai\n",
    "    GEMINI_AVAILABLE = True\n",
    "except ImportError:\n",
    "    GEMINI_AVAILABLE = False\n",
    "    \n",
    "import validator\n",
    "\n",
    "# =========================================================\n",
    "# 0. Prompt Definitions\n",
    "# =========================================================\n",
    "BASE_CHOREOGRAPHY_GEN_PROMPT = \"\"\"\n",
    "You are a choreographer AI. Generate a dance choreography for 2â€“5 dancers. \n",
    "Write it in natural English, describing entrances, movements, locations on stage (left, right, center, up-left, up-right, etc.) and give approximate times (in seconds). \n",
    "Keep it around 60â€“90 seconds.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "CHOREOGRAPHY_GEN_PROMPT = \"\"\"\n",
    "You are an expert Choreography Design System. Your goal is to create a conceptually rich and spatially complex dance piece for 2â€“5 dancers.\n",
    "\n",
    "To achieve high-quality output, follow this process:\n",
    "\n",
    "1. **Meta-Analysis & Setup:**\n",
    "   - define a specific dance style (e.g., Contemporary, Jazz, Ballet).\n",
    "   - Define the \"Mood\" and \"Narrative Arc\" of the piece.\n",
    "   - Assign specific roles or character dynamics to the dancers (e.g., Dancers A & B are mirrored, Dancer C is the disruptor).\n",
    "   - think step by step of the structure of the 60â€“90 seconds.\n",
    "   - Break it down into distinct sections\n",
    "   - Plan the spatial usage: Ensure dancers use the full stage.\n",
    "\n",
    "3. **Final Output Generation:**\n",
    "   - Based on the planning above, generate the final choreography script in natural English.\n",
    "   - Include precise timestamps\n",
    "   - Explicitly describe when each dancer **enters** and **exits** the stage.\n",
    "   - Movements should be physically possible (dancers cannot teleport across the stage instantly).\n",
    "   - Describe movements, specific stage locations, and interactions between dancers clearly.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "JSON_CONVERSION_TEMPLATE = \"\"\"\n",
    "Convert the choreography text into structured JSON. \n",
    "\n",
    "Rules: \n",
    "- Represent the stage as a 25x25 grid: x in [0..24], y in [0..24] \n",
    "- Include only events with fields: id, dancer, action, start, end, from, to, partner (optional) \n",
    "- Use seconds for start/end (float allowed) \n",
    "- If a movement has no clear start or end, infer them reasonably \n",
    "- Map narrative locations to grid coordinates: \n",
    "    \"left\"=0, \"right\"=24, \"center\"=12, \"up\"=0, \"down\"=24 \n",
    "    up-left=(0,0), up-right=(24,0), down-left=(0,24), down-right=(24,24)\n",
    "- **from** and **to** must be represented as dictionaries with x and y keys, for example: \n",
    "      \"from\": {\"x\": 0, \"y\": 2},\n",
    "      \"to\": {\"x\": 1, \"y\": 2}\n",
    "- If an action is for all dancers, repeat it for each dancer individually.\n",
    "\n",
    "- **IMPORTANT:** \n",
    "    1. Every dancer MUST have an explicit \"enter\" event (first event) and \"exit\" event (last event).\n",
    "    2. Dancers cannot move more than 4 cells in 1 second (speed limit).\n",
    "    3. Dancers cannot occupy the same (x,y) at the same time (collision).\n",
    "    4. Dancers cannot pause for more than 60 seconds.\n",
    "\n",
    "Return ONLY valid JSON in this format: \n",
    "{ \n",
    "  \"dancers\": [...], \n",
    "  \"duration\": ..., \n",
    "  \"events\": [...] \n",
    "}\n",
    "\n",
    "Choreography text: <<<TEXT_HERE>>>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 1. Filesystem helpers\n",
    "# =========================================================\n",
    "\n",
    "def ensure_dir(path: str) -> None:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def save_text(path: str, text: str) -> None:\n",
    "    ensure_dir(os.path.dirname(path))\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "\n",
    "def save_json(path: str, data: Any) -> None:\n",
    "    ensure_dir(os.path.dirname(path))\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "\n",
    "def make_run_folder(base_dir: str, model_name: str, run_id: str) -> str:\n",
    "    safe_model_name = model_name.replace(\"/\", \"-\")\n",
    "    run_dir = os.path.join(base_dir, \"logs\", safe_model_name, run_id)\n",
    "    ensure_dir(run_dir)\n",
    "    return run_dir\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 2. Model Detection & Client Initialization\n",
    "# =========================================================\n",
    "\n",
    "def is_gemini_model(model_name: str) -> bool:\n",
    "    \"\"\"Check if the model is a Gemini model.\"\"\"\n",
    "    return \"gemini\" in model_name.lower()\n",
    "\n",
    "\n",
    "def init_client(api_key: str, model_name: str):\n",
    "    \"\"\"\n",
    "    Initialize the appropriate client based on model type.\n",
    "    Returns (client, client_type) where client_type is 'gemini' or 'huggingface'\n",
    "    \"\"\"\n",
    "    if not api_key:\n",
    "        raise ValueError(\"API key is required.\")\n",
    "    \n",
    "    if is_gemini_model(model_name):\n",
    "        if not GEMINI_AVAILABLE:\n",
    "            raise ImportError(\"google-generativeai package not installed. Install with: pip install google-generativeai\")\n",
    "        genai.configure(api_key=api_key)\n",
    "        return genai.GenerativeModel(model_name), 'gemini'\n",
    "    else:\n",
    "        return InferenceClient(token=api_key), 'huggingface'\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 3. Unified API Call Functions\n",
    "# =========================================================\n",
    "\n",
    "from typing import Tuple, Optional\n",
    "import time\n",
    "\n",
    "def call_model(client, prompt: str, model_name: str, client_type: str, temperature: float = 0.7) -> str:\n",
    "    \"\"\"\n",
    "    Make a completion call using the appropriate client.\n",
    "    \"\"\"\n",
    "    if client_type == 'gemini':\n",
    "        # Gemini requires temperature to be passed in generation_config\n",
    "        response = client.generate_content(\n",
    "            prompt,\n",
    "            generation_config={\"temperature\": temperature}\n",
    "        )\n",
    "        return response.text\n",
    "    else:  # huggingface\n",
    "        response = client.chat_completion(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=6000, \n",
    "            temperature=temperature \n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def call_model_with_retry(\n",
    "    client,\n",
    "    prompt: str,\n",
    "    model_name: str,\n",
    "    client_type: str,\n",
    "    temperature: float = 1,  # Added temperature parameter here\n",
    "    max_retries: int = 5,\n",
    "    initial_backoff: float = 2.0,\n",
    ") -> Tuple[Optional[str], Optional[str]]:\n",
    "    \"\"\"\n",
    "    Call model with retry and exponential backoff.\n",
    "    Works with both Gemini and Hugging Face.\n",
    "    \"\"\"\n",
    "    backoff = initial_backoff\n",
    "    last_error = None\n",
    "\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            # Pass the temperature to the inner function\n",
    "            text = call_model(client, prompt, model_name, client_type, temperature=temperature)\n",
    "            return text, None\n",
    "        except Exception as e:\n",
    "            last_error = f\"Attempt {attempt} failed: {e}\"\n",
    "            print(last_error)\n",
    "            \n",
    "            # Handle model loading or rate limits\n",
    "            error_str = str(e).lower()\n",
    "            if \"loading\" in error_str or \"503\" in error_str or \"quota\" in error_str:\n",
    "                print(\"   (Model is loading or rate limited... waiting longer)\")\n",
    "                time.sleep(backoff * 2)\n",
    "            \n",
    "            if attempt < max_retries:\n",
    "                time.sleep(backoff)\n",
    "                backoff *= 2\n",
    "\n",
    "    return None, last_error\n",
    "\n",
    "# =========================================================\n",
    "# 4. JSON extraction + safe parsing\n",
    "# =========================================================\n",
    "\n",
    "def extract_json(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract the FIRST complete top-level JSON object using brace matching.\n",
    "    Handles truncation by returning the longest balanced substring.\n",
    "    \"\"\"\n",
    "    start = text.find(\"{\")\n",
    "    if start == -1:\n",
    "        return \"\"\n",
    "\n",
    "    depth = 0\n",
    "    end = None\n",
    "\n",
    "    for i in range(start, len(text)):\n",
    "        if text[i] == \"{\":\n",
    "            depth += 1\n",
    "        elif text[i] == \"}\":\n",
    "            depth -= 1\n",
    "            if depth == 0:\n",
    "                end = i\n",
    "                break\n",
    "\n",
    "    if end is None:\n",
    "        # JSON is truncated â€” return partial for repair\n",
    "        return text[start:].strip()\n",
    "\n",
    "    return text[start:end+1].strip()\n",
    "\n",
    "\n",
    "def is_truncated(json_str: str) -> bool:\n",
    "    \"\"\"\n",
    "    Detect if JSON is truncated by checking brace balance.\n",
    "    \"\"\"\n",
    "    return json_str.count(\"{\") != json_str.count(\"}\")\n",
    "\n",
    "\n",
    "def parse_json_safe(text: str) -> Tuple[Dict | None, str | None]:\n",
    "    cleaned = extract_json(text)\n",
    "\n",
    "    if is_truncated(cleaned):\n",
    "        return None, \"JSON appears truncated (unbalanced braces).\"\n",
    "\n",
    "    try:\n",
    "        parsed = json.loads(cleaned)\n",
    "        return parsed, None\n",
    "    except json.JSONDecodeError as e:\n",
    "        return None, f\"JSON parsing error at line {e.lineno}: {e.msg}\"\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 5. Generation functions\n",
    "# =========================================================\n",
    "\n",
    "def generate_choreography_text(client, model_name: str, client_type: str, temperature: float = 1) -> Tuple[str | None, str | None]:\n",
    "    text, err = call_model_with_retry(client=client, prompt=CHOREOGRAPHY_GEN_PROMPT, \n",
    "                                      model_name=model_name, client_type=client_type, temperature = temperature)\n",
    "    return text, err\n",
    "\n",
    "\n",
    "def generate_choreography_json(\n",
    "    text_choreography: str,\n",
    "    client,\n",
    "    model_name: str,\n",
    "    client_type: str, \n",
    "    temperature: float = 1\n",
    ") -> Tuple[Dict | None, str | None, str | None]:\n",
    "\n",
    "    prompt = JSON_CONVERSION_TEMPLATE.replace(\"<<<TEXT_HERE>>>\", text_choreography)\n",
    "\n",
    "    # First attempt\n",
    "    raw_json, err = call_model_with_retry(client=client, prompt=prompt, \n",
    "                                          model_name=model_name, client_type=client_type, temperature = temperature)\n",
    "    if err or raw_json is None:\n",
    "        return None, None, err\n",
    "    cleaned_json_str = extract_json(raw_json)\n",
    "    parsed, parse_err = parse_json_safe(cleaned_json_str)\n",
    "    return parsed, cleaned_json_str, parse_err\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 6. Validation & Refinement Logic\n",
    "# =========================================================\n",
    "\n",
    "def generate_feedback_prompt(violations: List[Dict]) -> str:\n",
    "    violation_summary = json.dumps(violations, indent=2)\n",
    "\n",
    "    return f\"\"\"\n",
    "The following choreography JSON contains validation errors. Fix them.\n",
    "\n",
    "Errors:\n",
    "{violation_summary}\n",
    "\n",
    "**Refinement Requirements:**\n",
    "- Correct every violation listed above.\n",
    "- Ensure all dancers have explicit 'enter' and 'exit' actions.\n",
    "- enteries and exits should be seperated from the rest of the dance they are place holders to track please pay attention.\n",
    "- Ensure no two dancers share the same (x,y) at the same time.\n",
    "- Ensure no dancer stays still for more than 60 seconds.\n",
    "- Keep all coordinates within the 0â€“24 stage boundaries.\n",
    "- Maintain chronological consistency.\n",
    "- Output ONLY valid JSON. No explanations, no markdown.\n",
    "- Ensure movement speed never exceeds 4 grid cells per second.\n",
    "\n",
    "Regenerate the full corrected JSON now.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def refine_choreography_loop(client, model_name: str, client_type: str, initial_json: Dict, \n",
    "                             run_dir: str, max_iterations: int = 10, temperature: float = 1) -> Dict:\n",
    "    current_json = initial_json\n",
    "    save_json(os.path.join(run_dir, \"iteration_0.json\"), current_json)\n",
    "\n",
    "    for i in range(1, max_iterations + 1):\n",
    "        print(f\"--- Validating Iteration {i-1} ---\")\n",
    "        report = validator.validate(current_json)\n",
    "        save_json(os.path.join(run_dir, f\"report_{i-1}.json\"), report)\n",
    "\n",
    "        if report[\"valid\"]:\n",
    "            print(f\"âœ… SUCCESS! Valid choreography found at iteration {i-1}\")\n",
    "            save_json(os.path.join(run_dir, \"final_valid.json\"), current_json)\n",
    "            return {\"success\": True, \"iterations\": i-1, \"final_json\": current_json}\n",
    "        \n",
    "        print(f\"âŒ Iteration {i-1} Failed. Violations: {len(report['violations'])}\")\n",
    "        feedback_prompt = generate_feedback_prompt(report[\"violations\"])\n",
    "        \n",
    "        full_refinement_prompt = f\"Current JSON:\\n{json.dumps(current_json)}\\n\\n{feedback_prompt}\"\n",
    "        \n",
    "        print(f\"--- Requesting Fix (Iteration {i}) ---\")\n",
    "        raw_text, err = call_model_with_retry(client, full_refinement_prompt, \n",
    "                                              model_name, client_type, temperature = temperature)\n",
    "        \n",
    "        if err:\n",
    "            print(f\"API Error during refinement: {err}\")\n",
    "            break\n",
    "            \n",
    "        new_json_str = extract_json(raw_text)\n",
    "        parsed, parse_err = parse_json_safe(new_json_str)\n",
    "        \n",
    "        if parse_err:\n",
    "            print(f\"JSON Parsing failed at iteration {i}: {parse_err}\")\n",
    "            save_text(os.path.join(run_dir, f\"failed_json_iter_{i}.txt\"), raw_text)\n",
    "            continue\n",
    "            \n",
    "        current_json = parsed\n",
    "        save_json(os.path.join(run_dir, f\"iteration_{i}.json\"), current_json)\n",
    "\n",
    "    print(\"âŒ Reached maximum iterations without success.\")\n",
    "    return {\"success\": False, \"iterations\": max_iterations, \"final_json\": current_json}\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 7. Main Pipeline\n",
    "# =========================================================\n",
    "\n",
    "def run_pipeline(api_key: str, model_name: str):\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    base_dir = \"modified_choreography_prompt\"\n",
    "    temperature = 0.1\n",
    "    temp_str = f\"temp_{temperature}\"\n",
    "    run_dir = make_run_folder(base_dir, model_name, f\"run_{temp_str}_{timestamp}\")\n",
    "    \n",
    "    print(f\"ðŸš€ Starting Run: {timestamp}\")\n",
    "    print(f\"ðŸ“‚ Output Directory: {run_dir}\")\n",
    "    print(f\"ðŸ¤– Model: {model_name}\")\n",
    "    \n",
    "    # Initialize the appropriate client\n",
    "    client, client_type = init_client(api_key, model_name)\n",
    "    print(f\"ðŸ”§ Using {client_type} client\")\n",
    "    # --- Step 1 ---\n",
    "    print(\"\\n[1/3] Generating Natural Language Choreography...\")\n",
    "    text_choreo, err = generate_choreography_text(client, model_name, client_type, temperature)\n",
    "    if err:\n",
    "        print(f\"Critical Error: {err}\")\n",
    "        return\n",
    "    print(\"   -> Text generated.\")\n",
    "    save_text(os.path.join(run_dir, \"choreography.txt\"), text_choreo)\n",
    "\n",
    "    # --- Step 2 ---\n",
    "    print(\"\\n[2/3] Converting to JSON...\")\n",
    "    initial_json, raw_json_str, parse_err = generate_choreography_json(\n",
    "        text_choreo, client, model_name, client_type, temperature)\n",
    "    if parse_err:\n",
    "        print(f\"Critical JSON Error: {parse_err}\")\n",
    "        save_text(os.path.join(run_dir, \"failed_initial_json.txt\"), raw_json_str)\n",
    "        return\n",
    "    print(\"   -> Initial JSON created.\")\n",
    "\n",
    "    # --- Step 3 ---\n",
    "    print(\"\\n[3/3] Entering Validation Loop...\")\n",
    "    result = refine_choreography_loop(client, model_name, client_type, initial_json, run_dir, max_iterations = 6, temperature = temperature)\n",
    "\n",
    "    # --- Summary ---\n",
    "    summary = {\n",
    "        \"model\": model_name,\n",
    "        \"timestamp\": timestamp,\n",
    "        \"status\": \"Success\" if result[\"success\"] else \"Failed\",\n",
    "        \"iterations_required\": result[\"iterations\"],\n",
    "        \"client_type\": client_type,\n",
    "        \"temperature\": temperature,\n",
    "    }\n",
    "    save_json(os.path.join(run_dir, \"summary.json\"), summary)\n",
    "    print(\"\\nDone.\")\n",
    "\n",
    "\n",
    "###########################\n",
    "\n",
    "API_KEY = \"gemini_API_KEY\"\n",
    "\n",
    "MODEL_NAME = \"gemini-2.5-flash-lite\"\n",
    "MODEL_NAME = \"gemini-2.5-flash\"\n",
    "MODEL_NAME = \"gemini-3-pro-preview\"\n",
    "\n",
    "\n",
    "run_pipeline(API_KEY, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2459c8a-1256-4fb0-b6cb-95e19ab5ff1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
